# Learning-ml-with-scikit-learn

**Project Overview**
- **Name:** Learning ML with Scikit-Learn — Hands-on Notebooks
- **Purpose:** A compact, practical collection of Jupyter notebooks that demonstrate key scikit-learn workflows: data analysis, preprocessing, modeling, evaluation and pipelines using the California housing and Iris datasets.

**Table of Contents**
- **Project:** Quick project summary and structure
- **Notebooks:** Short descriptions with links to files
- **Dataset:** `housing.csv` schema and notes
- **Requirements:** Recommended environment and install commands
- **How to run:** Open notebooks locally (VS Code / Jupyter)
- **Contribute:** Notes for contributions

**Project Structure (key files)**
- **Notebooks:**
  - `01_Scikit_learn_basic-Copy1.ipynb` — Basic scikit-learn example: DecisionTreeClassifier demo and tree plotting (toy fruit dataset).
  - `02_Using_different_ML_Models.ipynb` — Demonstrates using a different model (RandomForestClassifier) with the same toy dataset.
  - `03_Quick_training_on_iris-Copy1.ipynb` — Quick training workflow on the Iris dataset (train/test XLSX files referenced).
  - `04_Accuracy _on_iris.ipynb` — Compute and report model accuracy for Iris predictions.
  - `05_Analyzing_the_Data-Copy1.ipynb` — Exploratory Data Analysis (EDA) for `housing.csv`: head/tail/info/histograms.
  - `06_Creating_a_test_set-Copy1.ipynb` — Creating reproducible train/test splits (including stratified sampling by income category).
  - `07_visualizing_the_data-Copy1.ipynb` — Visualizations: scatter plots, density and correlation inspection, scatter matrix.
  - `08_Further_preprocessing-Copy1.ipynb` — Preprocessing steps: imputation with `SimpleImputer`, numeric pipelines, creating features/labels.
  - `09_Handling_Categorical-Copy1.ipynb` — Handling categorical attributes (OneHotEncoder example for `ocean_proximity`).
  - `10_Feature_scaling-Copy1.ipynb` — Feature scaling demos: `MinMaxScaler` and `StandardScaler`.
  - `11_Pipelines_in_sklearn.ipynb` — Example of `sklearn.pipeline.Pipeline` combining imputation and scaling.
- **Data:**
  - `housing.csv` — California housing dataset (columns documented below).

**Dataset: `housing.csv`**
- Typical columns present:
  - `longitude`, `latitude` — geolocation
  - `housing_median_age` — age of housing block
  - `total_rooms`, `total_bedrooms`, `population`, `households` — housing/people statistics
  - `median_income` — median income in block (used to create `income_cat` in notebooks)
  - `median_house_value` — target variable for regression tasks
  - `ocean_proximity` — categorical feature (e.g., `NEAR BAY`, `INLAND`, `<1H OCEAN`)

Notes: notebooks perform EDA, create `income_cat` bins, stratified split, and demonstrate preprocessing for `median_house_value` prediction.

**Quick Start (Windows PowerShell)**
- Recommended Python version: 3.8+ (3.9 or 3.10 recommended)
- Create and activate a conda environment (if you use Anaconda/Miniconda):
```
conda create -n sklearn-env python=3.9 -y
conda activate sklearn-env
```
- Install core packages with pip:
```
pip install --upgrade pip
pip install jupyterlab notebook pandas numpy matplotlib scikit-learn
```

**Open and run notebooks**
- In VS Code: open the folder `anaconda_projects/db`, click a `.ipynb` file and use the built-in notebook UI.
- In JupyterLab / Notebook (terminal):
```
jupyter lab
# or
jupyter notebook
```
- To run cells sequentially use the Run menu or keyboard shortcuts (Shift+Enter).

**How the README is interactive**
- Each notebook is listed with a short summary and path so you can click-open directly in GitHub/VS Code.
- The instructions contain ready-to-copy PowerShell commands for reproducible setup on Windows.

**Recommended workflow**
1. Create the environment (see Quick Start).
2. Open `05_Analyzing_the_Data-Copy1.ipynb` to inspect `housing.csv` and understand features.
3. Progress through the notebooks in order from data preparation (`06_...`) → preprocessing (`08_...`, `09_...`, `10_...`) → modeling and pipelines (`11_...`).

**Contributing**
- If you want to improve examples, add a new notebook and submit a PR. Keep notebooks focused and include small datasets or instructions to obtain them.

**Next steps I can help with**
- Add a `requirements.txt` or `environment.yml` for reproducible installs.
- Add a small Python runner script to reproduce selected notebook steps.

---
_Generated by a repository scan — created README summarises notebooks and dataset and includes Quick Start instructions._
